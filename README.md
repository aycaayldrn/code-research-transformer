# Embeddings-Based Code Search Engine (CoSQA Benchmark)

This project implements and evaluates an embeddings-based semantic code search engine. The goal was to build a retrieval system capable of matching natural language queries (like "function to sort an array") to the correct function body from a collection of code snippets.

The solution is divided into three parts: building a baseline search engine, evaluating its performance on the CoSQA test set, and finally, fine-tuning the model to demonstrate performance improvement.

---

## Project Structure and Files

The core project resides in the `report.ipynb` file, which serves as a live, executable demonstration and final report.

* `report.ipynb`: The primary file containing all code execution, analysis, and metric calculations (Part 1, Part 2, and Part 3).
* `cosqa_test_data.json`: The downloaded test set used for baseline and final evaluation.
* `cosqa_train_data.json`: The downloaded training set used for fine-tuning the model.
* `output/finetuned_cosqa_[TIMESTAMP]`: The directory containing the saved fine-tuned model (`model.safetensors`, `config.json`, etc.).

---

## Part 1: Architecture and Technical Decisions

The core architecture uses a dual-encoder approach: a Natural Language Query is encoded into a vector, and all Code Documents are pre-encoded into a searchable vector index.

### Model Selection: The Baseline and the Strategy

I selected the **`all-MiniLM-L6-v2`** model as the baseline model for its **efficiency and speed**. While it is a general-purpose semantic search model (not code-specific), it generates highly effective 384-dimensional embeddings very quickly. This allowed for rapid prototyping and, crucially, provided a clear, reproducible **baseline score** to compare against later. The expectation, which was confirmed in the evaluation, was that this model would struggle with the nuance of code syntax, paving the way for the necessary specialization in Part 3.

The **`sentence-transformers` library** was chosen to implement the model layer. This framework is purpose-built for generating sentence/text embeddings and simplifies loading pre-trained Hugging Face models, batch encoding, and, most importantly, the fine-tuning process.

### Vector Storage and Retrieval: `usearch`

For vector storage and retrieval, **`usearch`** was chosen over larger vector database systems (Qdrant, Weaviate) and even other in-memory libraries (Faiss). The actual difference lies in `usearch`'s **minimal overhead and foundational design**. While providing maximum retrieval speed via the HNSW index, it requires the **minimum necessary configuration** compared to larger databases, allowing the project to focus purely on the retrieval logic and benchmarking rather than environment management.

The search metric used was **Cosine Similarity**, as this is the standard metric for comparing the angular distance between vectors generated by `Sentence-Transformers`.

---

## Part 2: Evaluation and Baseline Metrics

The search engine was evaluated on the CoSQA test set (`test_webquery.json`), which consists of Natural Language Queries paired with their single correct Code Document from a large pool of candidates.

### Baseline Metrics (Before Fine-Tuning)

The initial metrics established a starting point:

| Metric | Score (k=10) |
| :--- | :--- |
| **Recall@10** | **0.5899** |
| **MRR@10** | **0.2959** |
| **NDCG@10** | **0.3654** |

The low MRR and NDCG scores confirmed the hypothesis: the general-purpose model often finds the correct answer (59% Recall), but struggles to rank it highly (MRR of $\sim 0.3$, meaning the average correct rank was close to Rank 3 or 4). This provided a clear opportunity for improvement.

---

## Part 3: Fine-Tuning and Performance Improvement

### Loss Function Selection - MNR Loss

This loss function is ideal for retrieval tasks because it operates contrastively. For every batch of training data, it treats the official paired (Query, Correct Code) as the positive example, and all other code snippets in that same batch are treated as "hard" negative examples. The loss function then forces the query's embedding closer to the positive code's embedding while simultaneously pushing it away from all the other negative code embeddings.

### Final Results and Demonstration of Improvement

The model was fine-tuned for just **one epoch** on the CoSQA training set (`cosqa-retrieval-train-19604.json`). The test set was then re-indexed using this new specialist model, and the evaluation was re-run.

The results clearly demonstrate that the fine-tuning successfully specialized the model, particularly improving the ranking quality (MRR and NDCG):

| Metric | Baseline (General) | Fine-Tuned (Specialist) | Absolute Improvement |
| :--- | :--- | :--- | :--- |
| **Recall@10** | 0.5899 | **0.6577** | **+0.0679** |
| **MRR@10** | 0.2959 | **0.3376** | **+0.0417** |
| **NDCG@10** | 0.3654 | **0.4140** | **+0.0486** |

The significant improvement in **MRR** and **NDCG** confirms that the fine-tuned model learned to better map the semantic intent of the natural language query to the subtle structural meaning of the code, resulting in the correct code snippet being consistently pushed closer to **Rank 1**.

## How to Run the Project

The entire solution, including all code, output, and analysis, is contained within the report.ipynb Jupyter Notebook.

The following packages are necessary to run the entire project pipeline (data fetching, embedding, indexing with usearch, and fine-tuning with sentence-transformers and PyTorch).
To fully be able to run the notebook, please first be sure that the listed libraries are installed:

* `%pip install sentence-transformers usearch numpy pandas requests`
* `%pip install accelerate>=0.26.0 # 'accelerate' is required by the Hugging Face Trainer (model.fit)`
* `%pip install datasets pyarrow`
* `%pip install torch`

